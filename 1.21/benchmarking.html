
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>NumPy benchmarks &#8212; NumPy v1.21 Manual</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.f6b7ca918bee2f46fd9abac01cfb07d5.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1e043a052b0af929e4d8.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Releasing a version" href="dev/releasing.html" />
    <link rel="prev" title="Reviewer Guidelines" href="dev/reviewer_guidelines.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
<link rel="stylesheet" href="_static/numpy.css" type="text/css" />

    <!-- PR #17220: This is added via javascript in versionwarning.js  -->
    <!-- link rel="canonical" href="http://numpy.org/doc/stable/benchmarking.html" / -->


  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">


    
      
      <a class="navbar-brand" href="index.html">
        <img src="_static/numpylogo.svg" class="logo" alt="logo">
      </a>      
      
    

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    
    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="user/index.html">
  User Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="reference/index.html">
  API reference
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="dev/index.html">
  Development
 </a>
</li>

        
      </ul>

      <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/numpy/numpy" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/numpy_team" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l2">
  <a class="reference internal" href="dev/gitwash/index.html">
   Git Basics
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/development_environment.html">
   Setting up and using your development environment
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/development_gitpod.html">
   Using Gitpod for NumPy development
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/development_workflow.html">
   Development workflow
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/development_advanced_debugging.html">
   Advanced debugging tools
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/reviewer_guidelines.html">
   Reviewer Guidelines
  </a>
 </li>
 <li class="toctree-l2 current active">
  <a class="current reference internal" href="#">
   NumPy benchmarks
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference external" href="https://numpy.org/neps/nep-0045-c_style_guide.html">
   NumPy C style guide
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/releasing.html">
   Releasing a version
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/governance/index.html">
   NumPy governance
  </a>
 </li>
 <li class="toctree-l2">
  <a class="reference internal" href="dev/howto-docs.html">
   How to contribute to the NumPy documentation
  </a>
 </li>
</ul>
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#usage">
   Usage
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#writing-benchmarks">
   Writing benchmarks
  </a>
 </li>
</ul>

</nav>


              
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="numpy-benchmarks">
<h1>NumPy benchmarks<a class="headerlink" href="#numpy-benchmarks" title="Permalink to this headline">¶</a></h1>
<p>Benchmarking NumPy with Airspeed Velocity.</p>
<div class="section" id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h2>
<p>Airspeed Velocity manages building and Python virtualenvs by itself,
unless told otherwise. Some of the benchmarking features in
<code class="docutils literal notranslate"><span class="pre">runtests.py</span></code> also tell ASV to use the NumPy compiled by
<code class="docutils literal notranslate"><span class="pre">runtests.py</span></code>. To run the benchmarks, you do not need to install a
development version of NumPy to your current Python environment.</p>
<p>Before beginning, ensure that <em>airspeed velocity</em> is installed.
By default, <em class="xref py py-obj">asv</em> ships with support for anaconda and virtualenv:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">asv</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">virtualenv</span>
</pre></div>
</div>
<p>After contributing new benchmarks, you should test them locally
before submitting a pull request.</p>
<p>To run all benchmarks, navigate to the root NumPy directory at
the command line and execute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">runtests</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">bench</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">--bench</span></code> activates the benchmark suite instead of the
test suite. This builds NumPy and runs  all available benchmarks
defined in <code class="docutils literal notranslate"><span class="pre">benchmarks/</span></code>. (Note: this could take a while. Each
benchmark is run multiple times to measure the distribution in
execution times.)</p>
<p>To run benchmarks from a particular benchmark module, such as
<code class="docutils literal notranslate"><span class="pre">bench_core.py</span></code>, simply append the filename without the extension:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">runtests</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">bench</span> <span class="n">bench_core</span>
</pre></div>
</div>
<p>To run a benchmark defined in a class, such as <code class="docutils literal notranslate"><span class="pre">Mandelbrot</span></code>
from <code class="docutils literal notranslate"><span class="pre">bench_avx.py</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">runtests</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">bench</span> <span class="n">bench_avx</span><span class="o">.</span><span class="n">Mandelbrot</span>
</pre></div>
</div>
<p>Compare change in benchmark results to another version/commit/branch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">runtests</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">bench</span><span class="o">-</span><span class="n">compare</span> <span class="n">v1</span><span class="o">.</span><span class="mf">6.2</span> <span class="n">bench_core</span>
<span class="n">python</span> <span class="n">runtests</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">bench</span><span class="o">-</span><span class="n">compare</span> <span class="mi">8</span><span class="n">bf4e9b</span> <span class="n">bench_core</span>
<span class="n">python</span> <span class="n">runtests</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">bench</span><span class="o">-</span><span class="n">compare</span> <span class="n">main</span> <span class="n">bench_core</span>
</pre></div>
</div>
<p>All of the commands above display the results in plain text in
the console, and the results are not saved for comparison with
future commits. For greater control, a graphical view, and to
have results saved for future comparison you can run ASV commands
(record results and generate HTML):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">benchmarks</span>
<span class="n">asv</span> <span class="n">run</span> <span class="o">-</span><span class="n">n</span> <span class="o">-</span><span class="n">e</span> <span class="o">--</span><span class="n">python</span><span class="o">=</span><span class="n">same</span>
<span class="n">asv</span> <span class="n">publish</span>
<span class="n">asv</span> <span class="n">preview</span>
</pre></div>
</div>
<p>More on how to use <code class="docutils literal notranslate"><span class="pre">asv</span></code> can be found in <a class="reference external" href="https://asv.readthedocs.io/">ASV documentation</a>
Command-line help is available as usual via <code class="docutils literal notranslate"><span class="pre">asv</span> <span class="pre">--help</span></code> and
<code class="docutils literal notranslate"><span class="pre">asv</span> <span class="pre">run</span> <span class="pre">--help</span></code>.</p>
</div>
<div class="section" id="writing-benchmarks">
<h2>Writing benchmarks<a class="headerlink" href="#writing-benchmarks" title="Permalink to this headline">¶</a></h2>
<p>See <a class="reference external" href="https://asv.readthedocs.io/">ASV documentation</a> for basics on how to write benchmarks.</p>
<p>Some things to consider:</p>
<ul class="simple">
<li><p>The benchmark suite should be importable with any NumPy version.</p></li>
<li><p>The benchmark parameters etc. should not depend on which NumPy version
is installed.</p></li>
<li><p>Try to keep the runtime of the benchmark reasonable.</p></li>
<li><p>Prefer ASV’s <code class="docutils literal notranslate"><span class="pre">time_</span></code> methods for benchmarking times rather than cooking up
time measurements via <code class="docutils literal notranslate"><span class="pre">time.clock</span></code>, even if it requires some juggling when
writing the benchmark.</p></li>
<li><p>Preparing arrays etc. should generally be put in the <code class="docutils literal notranslate"><span class="pre">setup</span></code> method rather
than the <code class="docutils literal notranslate"><span class="pre">time_</span></code> methods, to avoid counting preparation time together with
the time of the benchmarked operation.</p></li>
<li><p>Be mindful that large arrays created with <code class="docutils literal notranslate"><span class="pre">np.empty</span></code> or <code class="docutils literal notranslate"><span class="pre">np.zeros</span></code> might
not be allocated in physical memory until the memory is accessed. If this is
desired behaviour, make sure to comment it in your setup function. If
you are benchmarking an algorithm, it is unlikely that a user will be
executing said algorithm on a newly created empty/zero array. One can force
pagefaults to occur in the setup phase either by calling <code class="docutils literal notranslate"><span class="pre">np.ones</span></code> or
<code class="docutils literal notranslate"><span class="pre">arr.fill(value)</span></code> after creating the array,</p></li>
</ul>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="dev/reviewer_guidelines.html" title="previous page">Reviewer Guidelines</a>
    <a class='right-next' id="next-link" href="dev/releasing.html" title="next page">Releasing a version</a>

              </div>
              
          </main>
          

      </div>
    </div>

    
  <script src="_static/js/index.1e043a052b0af929e4d8.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2008-2021, The NumPy community.<br/>
        Last updated on Jun 22, 2021.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.0.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>